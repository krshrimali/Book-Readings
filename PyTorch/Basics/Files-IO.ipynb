{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/kushashwa/Pictures/\"\n",
    "files = glob.glob(os.path.join(path, '[k | K]*[.jpg | .png]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kushashwa/Pictures/kush_contours.png'"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[3] # 4th image is kush_contours.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = np.random.permutation(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kushashwa/Pictures/Krutika.png'"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clearly sixth image is kush_contours.png now.\n",
    "shuffle[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/Pictures/Krutika.png Krutika\n"
     ]
    }
   ],
   "source": [
    "for t in shuffle[5:6]:\n",
    "    folder = t.split('/')[-1].split('.')[0]\n",
    "    print(t, folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Pre-processing Image Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Resize all the images to the same size. Most of the deep learning architectures\n",
    "expect the images to be of the same size.\n",
    "2. Normalize the dataset with the mean and standard deviation of the dataset.\n",
    "3. Convert the image dataset to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "simple_transform = transforms.Compose([transforms.Scale((7, 7)), \n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize([0.485, 0.456,\n",
    "                                        0.406], [0.229, \\\n",
    "                                                0.224, 0.225])])\n",
    "train = ImageFolder('dogsandcats/', simple_transform)\n",
    "valid = ImageFolder('dogsandcats/', simple_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 37500\n",
       "    Root Location: dogsandcats/\n",
       "    Transforms (if any): Compose(\n",
       "                             Scale(size=(7, 7), interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 37500\n",
       "    Root Location: dogsandcats/\n",
       "    Transforms (if any): Compose(\n",
       "                             Scale(size=(7, 7), interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 0, 'valid': 1}"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'valid']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inpTensor):\n",
    "    '''\n",
    "    visualizes tensor\n",
    "    '''\n",
    "    inp = inpTensor.numpy().transpose((1, 2, 0))\n",
    "    print(inp.shape)\n",
    "    plt.imshow(inp)\n",
    "    plt.show()\n",
    "    print(\"After Normalizing\")\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std*inp + mean\n",
    "    # no need to clip, already normalized to (0, 1)\n",
    "    # inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><code>np.clip()</code></center>\n",
    "\n",
    "<center>**Source:** https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.clip.html</center>\n",
    "\n",
    "Given an interval, values outside the interval are clipped to the interval edges. For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleTensor = np.array([0.24, 200, 230, -10, 140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24, 2.  , 2.  , 0.  , 2.  ])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleTensor = np.clip(sampleTensor, 0, 2)\n",
    "sampleTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 7, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Normalizing\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshow(train[250][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = torch.utils.data.DataLoader(train, batch_size = 64, num_workers = 3)\n",
    "valid_data_gen = torch.utils.data.DataLoader(valid, batch_size = 64, num_workers = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 37500\n",
       "    Root Location: dogsandcats/\n",
       "    Transforms (if any): Compose(\n",
       "                             Scale(size=(7, 7), interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_gen.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 37500\n",
       "    Root Location: dogsandcats/\n",
       "    Transforms (if any): Compose(\n",
       "                             Scale(size=(7, 7), interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_gen.dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` class has 2 main methods (used mostly):\n",
    "\n",
    "1. `shuffle`: Shuffle the images on the dataset called.\n",
    "2. `num_workers`: For parallelization. Use number of workers < number of workers less than your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = model_fit.fc.in_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.fc.in_features = num_features\n",
    "model_fit.fc.out_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_fit.parameters(), lr = learning_rate, momentum = 0.9)\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size = 7, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a train_model function\n",
    "# tunes the weights (pre-trained weights)\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    dataloaders = {'train': train_data_gen, 'valid': valid_data_gen}\n",
    "    dataset_sizes = {'train': len(train_data_gen.dataset), 'valid': len(valid_data_gen.dataset)}\n",
    "    start = time.time()\n",
    "    # Returns a dictionary containing a whole state of the module.\n",
    "    # in the format ['weights', 'bias']\n",
    "    model_weights_bias = model.state_dict()\n",
    "    acc = 0.0 # starting accuracy\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch: {}/{}'.format(epoch, num_epochs-1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                # train when phase is training\n",
    "                scheduler.step()\n",
    "                model.train(True) # training mode\n",
    "            else:\n",
    "                # don't train when validation is going on\n",
    "                model.train(False) # evaluating mode\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "            \n",
    "            for data in dataloaders[phase]:\n",
    "                inputs, labels = data\n",
    "                \n",
    "                print(inputs.shape)\n",
    "                inputs, labels = Variable(inputs), Variable(labels) # not needed anymore\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                running_loss += loss.data[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            \n",
    "            if phase == 'valid' and epoch_acc > acc:\n",
    "                acc = epoch_acc\n",
    "                model_weights_bias = model.state_dict()\n",
    "        \n",
    "        end = time.time()\n",
    "        print(\"Time taken: \", end-start)\n",
    "        model.load_state_dict(model_weights_bias)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/24\n",
      "----------\n",
      "torch.Size([64, 3, 7, 7])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-243-ae712729cc75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-242-9cdb2b961748>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 547\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small at /pytorch/aten/src/THNN/generic/SpatialAveragePooling.c:64"
     ]
    }
   ],
   "source": [
    "train_model(model_fit, criterion, optimizer_ft, exp_lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Deep NN\n",
    "\n",
    "    1. Train the algorithm on the training dataset.\n",
    "    2. Perform Hyperparameter turning using validation dataset.\n",
    "    3. Perform fist 2 steps, iteratively until fixed number of epochs, or loss has reached to certain point.\n",
    "    4. Evaluate on test dataset.\n",
    "\n",
    "## Types of Splitting Methods\n",
    "\n",
    "    1. Simple holdout validation\n",
    "    2. K-fold validation\n",
    "    3. Iterative K-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Protocol\n",
    "\n",
    "There are three possible ways of evaluating our progress:\n",
    "\n",
    "    1. Holdout Validation Set\n",
    "        * When you have enough data \n",
    "    2. K-fold Cross Validation\n",
    "    3. Iterated k-fold Validation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConvNet-MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "fMk37rgj8mri",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Necessary Modules\n",
        "\n",
        "I'll be using PyTorch for implementing a sample Convolutional Neural Network on a two benchmark data sets for learning:\n",
        "\n",
        "1. MNIST Dataset\n",
        "2. Fashion MNIST Dataset (TODO)"
      ]
    },
    {
      "metadata": {
        "id": "J940WueO8lip",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader # For loading data\n",
        "from torchvision import transforms, utils\n",
        "from torchvision import datasets, models, transforms\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qFA_C3Q9CFd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load Data\n",
        "\n",
        "Let's go ahead and load MNIST data to training and validation data.\n",
        "\n",
        "I'll use PyTorch dataloader class for it."
      ]
    },
    {
      "metadata": {
        "id": "RK0vKRoD9A_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MNISTdata(Dataset):\n",
        "  \"\"\"\n",
        "  MNIST Dataset\n",
        "  ------\n",
        "  Parameters\n",
        "    :path: (string) [should be path to a folder]\n",
        "    :transform: (torchvision.transforms)\n",
        "    :batch_size: (int) default=8\n",
        "  \"\"\"\n",
        "  def __init__(self, transform, path=None, batch_size=8):\n",
        "    self.transforms = transform\n",
        "    self.bs = batch_size\n",
        "    \n",
        "    if path is not None:\n",
        "      self.path = path\n",
        "      self.train_dir = self.path + \"/train\"\n",
        "      self.valid_dir = self.path + \"/valid\"\n",
        "      self.data = {\n",
        "        'train': datasets.ImageFolder(root=self.train_dir, transform=self.transforms['train']),\n",
        "        'valid': datasets.ImageFolder(root=self.valid_dir, transform=self.transforms['valid']),\n",
        "      }\n",
        "    else:\n",
        "      train_dataset = torchvision.datasets.MNIST(root='data/',\n",
        "                                           train=True, \n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "      valid_dataset = torchvision.datasets.MNIST(root='data/',\n",
        "                                                train=False, \n",
        "                                                transform=transforms.ToTensor())\n",
        "      \n",
        "      self.data = {\n",
        "          'train': train_dataset,\n",
        "          'valid': valid_dataset\n",
        "      }\n",
        "    \n",
        "    self.train_loader = DataLoader(self.data['train'], batch_size=self.bs, shuffle=True)\n",
        "    self.valid_loader = DataLoader(self.data['valid'], batch_size=self.bs, shuffle=True)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.train_loader)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    return iter(self.train_loader)[idx]\n",
        "  \n",
        "  def get_loaders(self):\n",
        "    dataloaders = {\n",
        "        'train': self.train_loader,\n",
        "        'valid': self.valid_loader\n",
        "    }\n",
        "    return dataloaders\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0B0m26tUJoH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define Transforms for Data Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "CjOtQ6BtJb6K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2Mnmi1kKU6E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Define ConvNet Architecture"
      ]
    },
    {
      "metadata": {
        "id": "ek_aNK1UKXTy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Convolutional Neural Network Architecture\n",
        "  \"\"\"\n",
        "  def __init__(self, num_classes=10):\n",
        "    super(ConvNet, self).__init__()\n",
        "    self.layer1 = nn.Sequential(\n",
        "        nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
        "        nn.BatchNorm2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.layer2 = nn.Sequential(\n",
        "      nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "    self.fc = nn.Linear(7*7*32, num_classes)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = out.reshape(out.size(0),-1)\n",
        "    out = self.fc(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9hewi9MHLXEc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L5WYihT6LOYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "model = ConvNet(num_classes).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAeVh4KOLZbq",
        "colab_type": "code",
        "outputId": "844aac63-45dd-427d-b3fa-a35a29dd49f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "cell_type": "code",
      "source": [
        "data = MNISTdata(path=None, transform=image_transforms, batch_size=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:01, 8768621.50it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/28881 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 136020.26it/s]           \n",
            "  0%|          | 0/1648877 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2238158.86it/s]                            \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 51659.77it/s]            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a0vH3jGQMU4f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loaders = data.get_loaders()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jljue3h5MaNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training Model\n"
      ]
    },
    {
      "metadata": {
        "id": "rkrSfHJ0Mc2Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDjo027bMYNs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "  \"\"\"\n",
        "  Function to train and validate\n",
        "  \n",
        "  Parameters\n",
        "  :param model: Model to train and validate\n",
        "  :param loss_criterion: Loss Criterion to minimize\n",
        "  :param optimizer: Optimizer for computing gradients\n",
        "  :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "  Returns\n",
        "  model: Trained Model with best validation accuracy\n",
        "  history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "  \"\"\"\n",
        "  train_data = loaders['train']\n",
        "  valid_data = loaders['valid']\n",
        "  train_data_size = len(train_data)\n",
        "  valid_data_size = len(valid_data)\n",
        "  start = time.time()\n",
        "  \n",
        "  # Store best weights to best_weights\n",
        "  best_weights = copy.deepcopy(model.state_dict())\n",
        "  \n",
        "  # Initialize variables and lists\n",
        "  best_accuracy = 0.0\n",
        "  history_train_loss = []\n",
        "  history_val_loss = []\n",
        "  history_train_acc = []\n",
        "  history_val_acc = []\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "    # Set to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Variables for loss and accuracy of each epoch\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0\n",
        "    \n",
        "    # Iterate through the data\n",
        "    for _, data in enumerate(train_data):\n",
        "      inputs, labels = data\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      \n",
        "      # Clean the existing gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Calculate outputs on inputs data using the model\n",
        "      # And find loss and backpropagate\n",
        "      with torch.set_grad_enabled(True):\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_criterion(outputs, labels)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      # Find loss and accuracy\n",
        "      \n",
        "      train_loss += loss.item() * inputs.size(0)\n",
        "      \n",
        "      ret, predictions = torch.max(outputs.data, 1)\n",
        "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "      train_acc += acc.item() * inputs.size(0)\n",
        "    \n",
        "    # Find average training loss and training accuracy\n",
        "    avg_train_loss = train_loss/train_data_size \n",
        "    avg_train_acc = train_acc/float(train_data_size)\n",
        "    \n",
        "    epoch_end = time.time()\n",
        "    \n",
        "    print(\"Training: Loss: {:.4f}, Accuracy: {:.4f}, Time: {:.4f}s\".format(avg_train_loss, avg_train_acc, epoch_end-epoch_start))\n",
        "    \n",
        "    # Start validation \n",
        "    \n",
        "    # Set to evaluation mode\n",
        "    model.train(False)\n",
        "    model.eval()\n",
        "    \n",
        "    val_start = time.time()\n",
        "    \n",
        "    val_loss = 0.0\n",
        "    val_acc = 0\n",
        "    \n",
        "    # Iterate through the data\n",
        "    for _, data in enumerate(valid_data):\n",
        "      inputs, labels = data\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      \n",
        "      # Clean the existing gradients\n",
        "      optimizer.zero_grad()\n",
        "      \n",
        "      # Find outputs from the input data\n",
        "      outputs = model(inputs)\n",
        "      \n",
        "      # Find loss and accuracy\n",
        "      ret, predictions = torch.max(outputs.data, 1)\n",
        "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "      \n",
        "      val_loss += loss_criterion(outputs, labels).item() * inputs.size(0)\n",
        "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "      val_acc += acc.item() * inputs.size(0)\n",
        "    \n",
        "    # Calculate average validation loss and accuracy\n",
        "    avg_val_loss = val_loss/valid_data_size\n",
        "    avg_val_acc = val_acc/float(valid_data_size)\n",
        "    \n",
        "    val_end = time.time()\n",
        "    \n",
        "    history_train_loss.append(avg_train_loss)\n",
        "    history_val_loss.append(avg_val_loss)\n",
        "    \n",
        "    history_train_acc.append(avg_train_acc)\n",
        "    history_val_acc.append(avg_val_acc)\n",
        "    \n",
        "    print(\"Validation: Loss: {:.4f}, Accuracy: {:.4f}, Time: {:.4f}\\n\".format(avg_val_loss, avg_val_acc, val_end-val_start))\n",
        "    \n",
        "    # Save if the model has best accuracy till now\n",
        "    if(avg_val_acc > best_accuracy):\n",
        "      best_accuracy = avg_val_acc\n",
        "      best_weights = copy.deepcopy(model.state_dict())\n",
        "  \n",
        "  end = time.time()\n",
        "  \n",
        "  print(\"Total time elapsed: {:.4f}\".format(end-start))\n",
        "  print(\"Best Val accuracy: {:.4f}\".format(best_accuracy))\n",
        "  \n",
        "  model.load_state_dict(best_weights)\n",
        "  \n",
        "  history = { 'train_loss': history_train_loss, 'val_loss': history_val_loss, 'train_acc': history_train_acc, 'val_acc': history_val_acc}\n",
        "  return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xO_bLKmPNo4x",
        "colab_type": "code",
        "outputId": "ce729c60-ee28-4e42-da9e-6d040a96aebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "42aPxf8jMxxP",
        "colab_type": "code",
        "outputId": "4de8a0a5-9c2f-45bb-9734-edde4b047f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1830
        }
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "save_file_name = 'convnet.pth'\n",
        "checkpoint_path = 'convnet-checkpoint.pth'\n",
        "\n",
        "num_epochs = 25\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/25\n",
            "Training: Loss: 18.7002, Accuracy: 0.6595, Time: 23.7408s\n",
            "Validation: Loss: 18.7072, Accuracy: 0.6888, Time: 1.7544\n",
            "\n",
            "Epoch: 2/25\n",
            "Training: Loss: 18.6997, Accuracy: 0.6599, Time: 23.6475s\n",
            "Validation: Loss: 18.6971, Accuracy: 0.6824, Time: 2.0578\n",
            "\n",
            "Epoch: 3/25\n",
            "Training: Loss: 18.7011, Accuracy: 0.6584, Time: 25.1217s\n",
            "Validation: Loss: 18.6970, Accuracy: 0.6784, Time: 1.7597\n",
            "\n",
            "Epoch: 4/25\n",
            "Training: Loss: 18.6989, Accuracy: 0.6603, Time: 23.5196s\n",
            "Validation: Loss: 18.7030, Accuracy: 0.7040, Time: 1.7597\n",
            "\n",
            "Epoch: 5/25\n",
            "Training: Loss: 18.6998, Accuracy: 0.6601, Time: 23.5160s\n",
            "Validation: Loss: 18.7052, Accuracy: 0.6912, Time: 1.7620\n",
            "\n",
            "Epoch: 6/25\n",
            "Training: Loss: 18.7000, Accuracy: 0.6628, Time: 24.8605s\n",
            "Validation: Loss: 18.6959, Accuracy: 0.6720, Time: 1.7672\n",
            "\n",
            "Epoch: 7/25\n",
            "Training: Loss: 18.7004, Accuracy: 0.6616, Time: 23.5647s\n",
            "Validation: Loss: 18.7237, Accuracy: 0.6776, Time: 1.7580\n",
            "\n",
            "Epoch: 8/25\n",
            "Training: Loss: 18.7003, Accuracy: 0.6633, Time: 23.4870s\n",
            "Validation: Loss: 18.7021, Accuracy: 0.6792, Time: 1.7621\n",
            "\n",
            "Epoch: 9/25\n",
            "Training: Loss: 18.6977, Accuracy: 0.6708, Time: 26.3494s\n",
            "Validation: Loss: 18.7018, Accuracy: 0.6848, Time: 1.7686\n",
            "\n",
            "Epoch: 10/25\n",
            "Training: Loss: 18.7001, Accuracy: 0.6616, Time: 23.5047s\n",
            "Validation: Loss: 18.6960, Accuracy: 0.6648, Time: 1.7657\n",
            "\n",
            "Epoch: 11/25\n",
            "Training: Loss: 18.6996, Accuracy: 0.6612, Time: 23.5441s\n",
            "Validation: Loss: 18.7048, Accuracy: 0.6768, Time: 1.7596\n",
            "\n",
            "Epoch: 12/25\n",
            "Training: Loss: 18.7000, Accuracy: 0.6559, Time: 24.5978s\n",
            "Validation: Loss: 18.7014, Accuracy: 0.6888, Time: 2.0473\n",
            "\n",
            "Epoch: 13/25\n",
            "Training: Loss: 18.6998, Accuracy: 0.6549, Time: 23.5363s\n",
            "Validation: Loss: 18.7083, Accuracy: 0.6888, Time: 1.7632\n",
            "\n",
            "Epoch: 14/25\n",
            "Training: Loss: 18.6994, Accuracy: 0.6612, Time: 24.3521s\n",
            "Validation: Loss: 18.6903, Accuracy: 0.6808, Time: 1.7583\n",
            "\n",
            "Epoch: 15/25\n",
            "Training: Loss: 18.6998, Accuracy: 0.6655, Time: 24.2051s\n",
            "Validation: Loss: 18.6912, Accuracy: 0.6864, Time: 2.0417\n",
            "\n",
            "Epoch: 16/25\n",
            "Training: Loss: 18.6988, Accuracy: 0.6663, Time: 23.9518s\n",
            "Validation: Loss: 18.7068, Accuracy: 0.6744, Time: 1.7692\n",
            "\n",
            "Epoch: 17/25\n",
            "Training: Loss: 18.7002, Accuracy: 0.6595, Time: 23.5040s\n",
            "Validation: Loss: 18.7013, Accuracy: 0.6760, Time: 1.7672\n",
            "\n",
            "Epoch: 18/25\n",
            "Training: Loss: 18.6995, Accuracy: 0.6617, Time: 23.6513s\n",
            "Validation: Loss: 18.7038, Accuracy: 0.6832, Time: 2.0410\n",
            "\n",
            "Epoch: 19/25\n",
            "Training: Loss: 18.6993, Accuracy: 0.6583, Time: 24.3649s\n",
            "Validation: Loss: 18.7010, Accuracy: 0.6776, Time: 1.7561\n",
            "\n",
            "Epoch: 20/25\n",
            "Training: Loss: 18.6991, Accuracy: 0.6595, Time: 23.4965s\n",
            "Validation: Loss: 18.6967, Accuracy: 0.6856, Time: 1.7583\n",
            "\n",
            "Epoch: 21/25\n",
            "Training: Loss: 18.7003, Accuracy: 0.6611, Time: 25.1901s\n",
            "Validation: Loss: 18.6945, Accuracy: 0.6904, Time: 1.9410\n",
            "\n",
            "Epoch: 22/25\n",
            "Training: Loss: 18.7007, Accuracy: 0.6568, Time: 24.7087s\n",
            "Validation: Loss: 18.7096, Accuracy: 0.6824, Time: 1.7596\n",
            "\n",
            "Epoch: 23/25\n",
            "Training: Loss: 18.7004, Accuracy: 0.6632, Time: 23.5838s\n",
            "Validation: Loss: 18.7128, Accuracy: 0.6800, Time: 1.7705\n",
            "\n",
            "Epoch: 24/25\n",
            "Training: Loss: 18.7001, Accuracy: 0.6624, Time: 23.5415s\n",
            "Validation: Loss: 18.7128, Accuracy: 0.6808, Time: 1.7624\n",
            "\n",
            "Epoch: 25/25\n",
            "Training: Loss: 18.6991, Accuracy: 0.6617, Time: 24.8893s\n",
            "Validation: Loss: 18.7116, Accuracy: 0.6936, Time: 1.7691\n",
            "\n",
            "Total time elapsed: 647.8263\n",
            "Best Val accuracy: 0.7040\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
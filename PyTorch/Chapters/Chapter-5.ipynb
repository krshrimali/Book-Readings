{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Computer Vision\n",
    "\n",
    "Topics Covered:\n",
    "\n",
    "    1. \n",
    "    2. \n",
    "    3. \n",
    "    4. \n",
    "    5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import torch, torchvision\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges of using Linear Layers/FCN Layers?\n",
    "\n",
    "* Loss of Spatial Information.\n",
    "    * Flattening an image into 1D array loses all the spatial information about the image.\n",
    "* High complexity (number of weights for all layers in FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Image Classifier on MNIST Dataset\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Getting the Data\n",
    "2. Pre-process (flattening, resizing -- if required, split the dataset - train_test_split)\n",
    "3. Build model (CNN)\n",
    "4. Train and Validate the model.\n",
    "5. Test on Unseen Data (Test Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-1: Getting the Data**\n",
    "\n",
    "```python\n",
    "help(torchvision.datasets)\n",
    "```\n",
    "Output:\n",
    "```\n",
    "Help on package torchvision.datasets in torchvision:\n",
    "\n",
    "NAME\n",
    "    torchvision.datasets\n",
    "\n",
    "PACKAGE CONTENTS\n",
    "    cifar\n",
    "    coco\n",
    "    fakedata\n",
    "    folder\n",
    "    lsun\n",
    "    mnist\n",
    "    omniglot\n",
    "    phototour\n",
    "    semeion\n",
    "    stl10\n",
    "    svhn\n",
    "    utils\n",
    "```\n",
    "\n",
    "It's clear that we have mnist dataset available in torchvision datasets utility class. Let's go ahead and import mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required transformations\n",
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307, ), \\\n",
    "                                                                                (0.3081, ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = mnist.MNIST('data/', train=True, transform=transformation, download=True)\n",
    "test_dataset = mnist.MNIST('data/', train=False, transform=transformation, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to convert a tensor into an image\n",
    "def show_image(tensor, cmap=None):\n",
    "    image = tensor.numpy()[0]\n",
    "    mean = 0.1307\n",
    "    std_dev = 0.3081\n",
    "    image = ((mean * image) + std_dev)\n",
    "    plt.imshow(image, cmap) # show gray-scaled version of the image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0] # returns batch of 32 images, randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing image in color mode\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing image in grayscale mode\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Showing image in color mode\")\n",
    "show_image(next(iter(train_loader))[0][0])\n",
    "print(\"Showing image in grayscale mode\")\n",
    "show_image(next(iter(train_loader))[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model\n",
    "\n",
    "Generally, a CNN is composed of following layers:\n",
    "\n",
    "1. Conv2d\n",
    "2. MaxPooling2d\n",
    "3. ReLU\n",
    "4. View\n",
    "5. Linear Layer (FC Layer)\n",
    "6. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2d_dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.max_pool2d(self.conv2d_dropout(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 320) # why?\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Convolution works in PyTorch?\n",
    "\n",
    "Take an example of a 1D array. Let's try to apply a conv filter on the array.\n",
    "\n",
    "```\n",
    "conv = nn.Conv1d(1, 1, 3, bias=False)\n",
    "```\n",
    "\n",
    "Let's look at the docs of `nn.Conv1d`:\n",
    "\n",
    "```docs\n",
    "class Conv1d(_ConvNd)\n",
    " |  Applies a 1D convolution over an input signal composed of several input\n",
    " |  planes.\n",
    " |  \n",
    " |  In the simplest case, the output value of the layer with input size\n",
    " |  (N, C_{in}, L) and output (N, C_{out}, L_{out}) can be\n",
    " |  precisely described as:\n",
    "```\n",
    "$$\n",
    "\\begin{equation*}\n",
    "\\text{out}(N_i, C_{out_j}) = \\text{bias}(C_{out_j}) + \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{out_j}, k) \\star \\text{input}(N_i, k)\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "```docs\n",
    " |  Attributes:\n",
    " |      weight (Tensor): the learnable weights of the module of shape\n",
    " |          (out_channels, in_channels, kernel_size)\n",
    " |      bias (Tensor):   the learnable bias of the module of shape\n",
    " |          (out_channels)\n",
    " |  \n",
    " |  Examples::\n",
    " |  \n",
    " |      >>> m = nn.Conv1d(16, 33, 3, stride=2)\n",
    " |      >>> input = torch.randn(20, 16, 50)\n",
    " |      >>> output = m(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net.weight:  tensor([ 0.0898, -0.0290, -0.0184], grad_fn=<SelectBackward>)\n",
      "Net bias:  tensor(-0.0234, grad_fn=<AliasBackward>)\n",
      "Shape of input tensor: torch.Size([20, 16, 50])\n",
      "Input tensor([ 0.4412,  1.1871,  1.2538,  0.9038, -0.4481,  1.3991, -0.9111,  0.2678,\n",
      "         1.1055,  1.9549, -1.1615, -1.0567,  0.5989,  0.1253,  0.7138,  1.2872,\n",
      "         1.2061, -0.4276, -1.0316, -2.1270,  0.6886,  0.5826, -1.3476,  0.6016,\n",
      "        -0.3757, -1.1891,  0.6825, -0.0293, -1.0270,  0.8466,  1.6183, -1.1577,\n",
      "        -1.6786,  1.4142, -0.6939, -0.8775, -1.0439,  0.1629,  1.2870, -1.2488,\n",
      "        -0.8653,  0.8719,  0.2489, -2.0293, -2.2702,  0.8735,  0.9727,  0.5769,\n",
      "        -0.4099, -0.9260])\n",
      "output tensor([-0.4045,  0.4693, -0.6951, -0.4687, -0.0146, -0.4754,  0.5161, -0.1440,\n",
      "        -0.2339, -0.2184,  0.3562, -0.5512, -0.7846, -0.8485, -0.1358,  0.3537,\n",
      "        -0.2544,  0.1960, -0.3285,  0.1912, -0.1416,  0.0757,  0.1543,  1.5208],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv1d(16, 33, 3, stride=2)\n",
    "print(\"Net.weight: \", net.weight[0][0])\n",
    "print(\"Net bias: \", net.bias[0][0])\n",
    "input_ = torch.randn(20, 16, 50)\n",
    "print(\"Shape of input tensor: {}\".format(input_.shape))\n",
    "output = net(input_)\n",
    "print(\"Input\", input_[0][0])\n",
    "print(\"output\", output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0673,  0.1119, -0.0590], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0898, -0.0290, -0.0184], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0234, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0396, -0.0344, -0.0231], grad_fn=<ThMulBackward>)\n",
      "input_, net.weight, net.bias: tensor([0.4412, 1.1871, 1.2538])/tensor([ 0.0898, -0.0290, -0.0184], grad_fn=<SelectBackward>)/-0.023443467915058136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(input_[0][0][:3] * net.weight[0][0])\n",
    "print(\"input_, net.weight, net.bias: {}/{}/{}\".format(input_[0][0][:3], net.weight[0][0], net.bias[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.041310795"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(input_[0][0][:3].detach().numpy(), net.weight[0][0].detach().numpy()) + net.bias[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Pooling?\n",
    "\n",
    "Used just after a convolution layer, to reduce the data size to process. Also helps in reducing the size of feature maps. Also, forces algorithm to not focus on small changes in position. **(how?)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why View? \n",
    "\n",
    "Generally, we use `torch.Tensor.view()` (https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) function at the end of a network, because for FC (Fully Connected) or Linear layers, we need to flatten the data to 1D.\n",
    "\n",
    "While flattening, it's important to make sure that two different images don't mix-up. That's why we input the first argument to `torch.Tensor.view()` as `-1`.\n",
    "\n",
    "Let's look at the docs of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "view(...)\n",
      "    view(*args) -> Tensor\n",
      "    \n",
      "    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "    different size.\n",
      "    \n",
      "    The returned tensor shares the same data and must have the same number\n",
      "    of elements, but may have a different size. For a tensor to be viewed, the new\n",
      "    view size must be compatible with its original size and stride, i.e., each new\n",
      "    view dimension must either be a subspace of an original dimension, or only span\n",
      "    across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      "    contiguity-like condition that :math:`\\forall i = 0, \\dots, k-1`,\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "      stride[i] = stride[i+1] \\times size[i+1]\n",
      "    \n",
      "    Otherwise, :func:`contiguous` needs to be called before the tensor can be\n",
      "    viewed.\n",
      "    \n",
      "    Args:\n",
      "        args (torch.Size or int...): the desired size\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(4, 4)\n",
      "        >>> x.size()\n",
      "        torch.Size([4, 4])\n",
      "        >>> y = x.view(16)\n",
      "        >>> y.size()\n",
      "        torch.Size([16])\n",
      "        >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      "        >>> z.size()\n",
      "        torch.Size([2, 8])\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(torch.Tensor.view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor.view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[8], [9]], [[2], [0.3]]], [[[8], [9]], [[2], [0.3]]]])\n",
    "x = x.reshape((2, 1, 2, 2)) # reshape to (2, 1, 2, 2) - like a batch of 2 images of size (2x2) with 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[8. , 9. ],\n",
       "         [2. , 0.3]]],\n",
       "\n",
       "\n",
       "       [[[8. , 9. ],\n",
       "         [2. , 0.3]]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view() # view the array x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.from_numpy(x) # conversion of numpy array to a pytorch tensor\n",
    "# reference: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#converting-numpy-array-to-torch-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor.shape) # verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0000, 9.0000, 2.0000, 0.3000],\n",
       "        [8.0000, 9.0000, 2.0000, 0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.view(-1, 4) \n",
    "# -1 means: don't touch the first dimension (only if the second dimension satisfies the total elements)\n",
    "# if doesn't satisfy, then -1 is inferred from other dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0000, 9.0000],\n",
       "        [2.0000, 0.3000],\n",
       "        [8.0000, 9.0000],\n",
       "        [2.0000, 0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.view(-1, 2) # example: this should return (4, 2) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0000],\n",
       "        [9.0000],\n",
       "        [2.0000],\n",
       "        [0.3000],\n",
       "        [8.0000],\n",
       "        [9.0000],\n",
       "        [2.0000],\n",
       "        [0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.view(-1, 1) # example: this should return (8, 1) tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch, optimizer, model, data_loader, volatile=False, phase='training'):\n",
    "    if(phase == 'training'):\n",
    "        model.train()\n",
    "    if(phase == 'evaluation'):\n",
    "        model.evaluate()\n",
    "        volatile=True #why?\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "#         print(data)\n",
    "#         if data.is_cuda():\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile), Variable(target)\n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct/len(data_loader.dataset)\n",
    "    print(\"Loss: {}, accuracy: {}\".format(loss, accuracy))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0.5\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1206219345331192, accuracy: 96\n",
      "Loss: 0.12402940541505814, accuracy: 96\n",
      "Loss: 0.11611782014369965, accuracy: 96\n",
      "Loss: 0.11210104078054428, accuracy: 96\n",
      "Loss: 0.11929728090763092, accuracy: 96\n",
      "Loss: 0.1225995272397995, accuracy: 96\n",
      "Loss: 0.11394266784191132, accuracy: 96\n",
      "Loss: 0.11298033595085144, accuracy: 96\n",
      "Loss: 0.11729313433170319, accuracy: 96\n",
      "Loss: 0.11354488879442215, accuracy: 96\n",
      "Loss: 0.115715891122818, accuracy: 96\n",
      "Loss: 0.11816882342100143, accuracy: 96\n",
      "Loss: 0.11266960203647614, accuracy: 96\n",
      "Loss: 0.11292938143014908, accuracy: 96\n",
      "Loss: 0.11479900777339935, accuracy: 96\n",
      "Loss: 0.12624315917491913, accuracy: 96\n",
      "Loss: 0.11431062966585159, accuracy: 96\n",
      "Loss: 0.1180247887969017, accuracy: 96\n",
      "Loss: 0.1105051189661026, accuracy: 96\n",
      "Loss: 0.12324003875255585, accuracy: 96\n",
      "Loss: 0.11057820171117783, accuracy: 96\n",
      "Loss: 0.10865706950426102, accuracy: 97\n",
      "Loss: 0.10824849456548691, accuracy: 96\n",
      "Loss: 0.11275696009397507, accuracy: 96\n",
      "Loss: 0.10923698544502258, accuracy: 96\n",
      "Loss: 0.11321546882390976, accuracy: 96\n",
      "Loss: 0.10587245225906372, accuracy: 96\n",
      "Loss: 0.11644107848405838, accuracy: 96\n",
      "Loss: 0.10572720319032669, accuracy: 96\n",
      "Loss: 0.10592672973871231, accuracy: 96\n",
      "Loss: 0.10624512284994125, accuracy: 96\n",
      "Loss: 0.12218598276376724, accuracy: 96\n",
      "Loss: 0.1034180149435997, accuracy: 96\n",
      "Loss: 0.114189513027668, accuracy: 96\n",
      "Loss: 0.10513007640838623, accuracy: 96\n",
      "Loss: 0.11694205552339554, accuracy: 96\n",
      "Loss: 0.10253959894180298, accuracy: 96\n",
      "Loss: 0.10903823375701904, accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, optimizer, model, train_loader, volatile=False, phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,optimizer, model, test_loader, volatile=False, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa4d4396940>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\n",
    "plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

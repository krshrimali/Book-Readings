{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning in Computer Vision\n",
    "\n",
    "Topics Covered:\n",
    "\n",
    "1. Building Image Classifier on MNIST Dataset\n",
    "2. Working of Conv in PyTorch\n",
    "3. Why Pooling?\n",
    "4. Why use View?\n",
    "5. Transfer Learning\n",
    "6. Visualizing Outputs from Intermediate Layers\n",
    "7. Visualizing Weights of Intermediate Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import torch, torchvision\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges of using Linear Layers/FCN Layers?\n",
    "\n",
    "* Loss of Spatial Information.\n",
    "    * Flattening an image into 1D array loses all the spatial information about the image.\n",
    "* High complexity (number of weights for all layers in FCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Image Classifier on MNIST Dataset\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Getting the Data\n",
    "2. Pre-process (flattening, resizing -- if required, split the dataset - train_test_split)\n",
    "3. Build model (CNN)\n",
    "4. Train and Validate the model.\n",
    "5. Test on Unseen Data (Test Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step-1: Getting the Data**\n",
    "\n",
    "```python\n",
    "help(torchvision.datasets)\n",
    "```\n",
    "Output:\n",
    "```\n",
    "Help on package torchvision.datasets in torchvision:\n",
    "\n",
    "NAME\n",
    "    torchvision.datasets\n",
    "\n",
    "PACKAGE CONTENTS\n",
    "    cifar\n",
    "    coco\n",
    "    fakedata\n",
    "    folder\n",
    "    lsun\n",
    "    mnist\n",
    "    omniglot\n",
    "    phototour\n",
    "    semeion\n",
    "    stl10\n",
    "    svhn\n",
    "    utils\n",
    "```\n",
    "\n",
    "It's clear that we have mnist dataset available in torchvision datasets utility class. Let's go ahead and import mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets.mnist as mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required transformations\n",
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307, ), \\\n",
    "                                                                                (0.3081, ))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = mnist.MNIST('data/', train=True, transform=transformation, download=True)\n",
    "test_dataset = mnist.MNIST('data/', train=False, transform=transformation, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to convert a tensor into an image\n",
    "def show_image(tensor, cmap=None):\n",
    "    image = tensor.numpy()[0]\n",
    "    mean = 0.1307\n",
    "    std_dev = 0.3081\n",
    "    image = ((mean * image) + std_dev)\n",
    "    plt.imshow(image, cmap) # show gray-scaled version of the image\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0] # returns batch of 32 images, randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing image in color mode\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing image in grayscale mode\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Showing image in color mode\")\n",
    "show_image(next(iter(train_loader))[0][0])\n",
    "print(\"Showing image in grayscale mode\")\n",
    "show_image(next(iter(train_loader))[0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model\n",
    "\n",
    "Generally, a CNN is composed of following layers:\n",
    "\n",
    "1. Conv2d\n",
    "2. MaxPooling2d\n",
    "3. ReLU\n",
    "4. View\n",
    "5. Linear Layer (FC Layer)\n",
    "6. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2d_dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.max_pool2d(self.conv2d_dropout(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 320) # why?\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Convolution works in PyTorch?\n",
    "\n",
    "Take an example of a 1D array. Let's try to apply a conv filter on the array.\n",
    "\n",
    "```\n",
    "conv = nn.Conv1d(1, 1, 3, bias=False)\n",
    "```\n",
    "\n",
    "Let's look at the docs of `nn.Conv1d`:\n",
    "\n",
    "```docs\n",
    "class Conv1d(_ConvNd)\n",
    " |  Applies a 1D convolution over an input signal composed of several input\n",
    " |  planes.\n",
    " |  \n",
    " |  In the simplest case, the output value of the layer with input size\n",
    " |  (N, C_{in}, L) and output (N, C_{out}, L_{out}) can be\n",
    " |  precisely described as:\n",
    "```\n",
    "$$\n",
    "\\text{out}(N_i, C_{out_j}) = \\text{bias}(C_{out_j}) + \\sum_{k = 0}^{C_{in} - 1} \\text{weight}(C_{out_j}, k) \\star \\text{input}(N_i, k)\n",
    "$$\n",
    "\n",
    "```docs\n",
    " |  Attributes:\n",
    " |      weight (Tensor): the learnable weights of the module of shape\n",
    " |          (out_channels, in_channels, kernel_size)\n",
    " |      bias (Tensor):   the learnable bias of the module of shape\n",
    " |          (out_channels)\n",
    " |  \n",
    " |  Examples::\n",
    " |  \n",
    " |      >>> m = nn.Conv1d(16, 33, 3, stride=2)\n",
    " |      >>> input = torch.randn(20, 16, 50)\n",
    " |      >>> output = m(input)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net.weight:  tensor([ 0.0898, -0.0290, -0.0184], grad_fn=<SelectBackward>)\n",
      "Net bias:  tensor(-0.0234, grad_fn=<AliasBackward>)\n",
      "Shape of input tensor: torch.Size([20, 16, 50])\n",
      "Input tensor([ 0.4412,  1.1871,  1.2538,  0.9038, -0.4481,  1.3991, -0.9111,  0.2678,\n",
      "         1.1055,  1.9549, -1.1615, -1.0567,  0.5989,  0.1253,  0.7138,  1.2872,\n",
      "         1.2061, -0.4276, -1.0316, -2.1270,  0.6886,  0.5826, -1.3476,  0.6016,\n",
      "        -0.3757, -1.1891,  0.6825, -0.0293, -1.0270,  0.8466,  1.6183, -1.1577,\n",
      "        -1.6786,  1.4142, -0.6939, -0.8775, -1.0439,  0.1629,  1.2870, -1.2488,\n",
      "        -0.8653,  0.8719,  0.2489, -2.0293, -2.2702,  0.8735,  0.9727,  0.5769,\n",
      "        -0.4099, -0.9260])\n",
      "output tensor([-0.4045,  0.4693, -0.6951, -0.4687, -0.0146, -0.4754,  0.5161, -0.1440,\n",
      "        -0.2339, -0.2184,  0.3562, -0.5512, -0.7846, -0.8485, -0.1358,  0.3537,\n",
      "        -0.2544,  0.1960, -0.3285,  0.1912, -0.1416,  0.0757,  0.1543,  1.5208],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "net = nn.Conv1d(16, 33, 3, stride=2)\n",
    "print(\"Net.weight: \", net.weight[0][0])\n",
    "print(\"Net bias: \", net.bias[0][0])\n",
    "input_ = torch.randn(20, 16, 50)\n",
    "print(\"Shape of input tensor: {}\".format(input_.shape))\n",
    "output = net(input_)\n",
    "print(\"Input\", input_[0][0])\n",
    "print(\"output\", output[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0673,  0.1119, -0.0590], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0898, -0.0290, -0.0184], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.weight[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0234, grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bias[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0396, -0.0344, -0.0231], grad_fn=<ThMulBackward>)\n",
      "input_, net.weight, net.bias: tensor([0.4412, 1.1871, 1.2538])/tensor([ 0.0898, -0.0290, -0.0184], grad_fn=<SelectBackward>)/-0.023443467915058136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print(input_[0][0][:3] * net.weight[0][0])\n",
    "print(\"input_, net.weight, net.bias: {}/{}/{}\".format(input_[0][0][:3], net.weight[0][0], net.bias[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.041310795"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(input_[0][0][:3].detach().numpy(), net.weight[0][0].detach().numpy()) + net.bias[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parameter.Parameter"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(net.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Pooling?\n",
    "\n",
    "Used just after a convolution layer, to reduce the data size to process. Also helps in reducing the size of feature maps. Also, forces algorithm to not focus on small changes in position. **(how?)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why View? \n",
    "\n",
    "Generally, we use `torch.Tensor.view()` (https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view) function at the end of a network, because for FC (Fully Connected) or Linear layers, we need to flatten the data to 1D.\n",
    "\n",
    "While flattening, it's important to make sure that two different images don't mix-up. That's why we input the first argument to `torch.Tensor.view()` as `-1`.\n",
    "\n",
    "Let's look at the docs of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method_descriptor:\n",
      "\n",
      "view(...)\n",
      "    view(*args) -> Tensor\n",
      "    \n",
      "    Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      "    different size.\n",
      "    \n",
      "    The returned tensor shares the same data and must have the same number\n",
      "    of elements, but may have a different size. For a tensor to be viewed, the new\n",
      "    view size must be compatible with its original size and stride, i.e., each new\n",
      "    view dimension must either be a subspace of an original dimension, or only span\n",
      "    across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      "    contiguity-like condition that :math:`\\forall i = 0, \\dots, k-1`,\n",
      "    \n",
      "    .. math::\n",
      "    \n",
      "      stride[i] = stride[i+1] \\times size[i+1]\n",
      "    \n",
      "    Otherwise, :func:`contiguous` needs to be called before the tensor can be\n",
      "    viewed.\n",
      "    \n",
      "    Args:\n",
      "        args (torch.Size or int...): the desired size\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> x = torch.randn(4, 4)\n",
      "        >>> x.size()\n",
      "        torch.Size([4, 4])\n",
      "        >>> y = x.view(16)\n",
      "        >>> y.size()\n",
      "        torch.Size([16])\n",
      "        >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      "        >>> z.size()\n",
      "        torch.Size([2, 8])\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(torch.Tensor.view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor.view?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[[8], [9]], [[2], [0.3]]], [[[8], [9]], [[2], [0.3]]]])\n",
    "x = x.reshape((2, 1, 2, 2)) # reshape to (2, 1, 2, 2) - like a batch of 2 images of size (2x2) with 1 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[8. , 9. ],\n",
       "         [2. , 0.3]]],\n",
       "\n",
       "\n",
       "       [[[8. , 9. ],\n",
       "         [2. , 0.3]]]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view() # view the array x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.from_numpy(x) # conversion of numpy array to a pytorch tensor\n",
    "# reference: https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#converting-numpy-array-to-torch-tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x_tensor.shape) # verifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0000, 9.0000, 2.0000, 0.3000],\n",
       "        [8.0000, 9.0000, 2.0000, 0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.view(-1, 4) \n",
    "# -1 means: don't touch the first dimension (only if the second dimension satisfies the total elements)\n",
    "# if doesn't satisfy, then -1 is inferred from other dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0000, 9.0000],\n",
       "        [2.0000, 0.3000],\n",
       "        [8.0000, 9.0000],\n",
       "        [2.0000, 0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.view(-1, 2) # example: this should return (4, 2) tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.0000],\n",
       "        [9.0000],\n",
       "        [2.0000],\n",
       "        [0.3000],\n",
       "        [8.0000],\n",
       "        [9.0000],\n",
       "        [2.0000],\n",
       "        [0.3000]], dtype=torch.float64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.view(-1, 1) # example: this should return (8, 1) tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epoch, optimizer, model, data_loader, volatile=False, phase='training'):\n",
    "    if(phase == 'training'):\n",
    "        model.train()\n",
    "    if(phase == 'evaluation'):\n",
    "        model.evaluate()\n",
    "        volatile=True #why?\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "#         print(data)\n",
    "#         if data.is_cuda():\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile), Variable(target)\n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        running_loss += F.nll_loss(output, target, size_average=False).data[0]\n",
    "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
    "        running_correct += preds.eq(target.data.view_as(preds)).cpu().sum()\n",
    "        if phase == 'training':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * running_correct/len(data_loader.dataset)\n",
    "    print(\"Loss: {}, accuracy: {}\".format(loss, accuracy))\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0.5\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_accuracy = [], []\n",
    "val_losses, val_accuracy = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/kushashwa/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1206219345331192, accuracy: 96\n",
      "Loss: 0.12402940541505814, accuracy: 96\n",
      "Loss: 0.11611782014369965, accuracy: 96\n",
      "Loss: 0.11210104078054428, accuracy: 96\n",
      "Loss: 0.11929728090763092, accuracy: 96\n",
      "Loss: 0.1225995272397995, accuracy: 96\n",
      "Loss: 0.11394266784191132, accuracy: 96\n",
      "Loss: 0.11298033595085144, accuracy: 96\n",
      "Loss: 0.11729313433170319, accuracy: 96\n",
      "Loss: 0.11354488879442215, accuracy: 96\n",
      "Loss: 0.115715891122818, accuracy: 96\n",
      "Loss: 0.11816882342100143, accuracy: 96\n",
      "Loss: 0.11266960203647614, accuracy: 96\n",
      "Loss: 0.11292938143014908, accuracy: 96\n",
      "Loss: 0.11479900777339935, accuracy: 96\n",
      "Loss: 0.12624315917491913, accuracy: 96\n",
      "Loss: 0.11431062966585159, accuracy: 96\n",
      "Loss: 0.1180247887969017, accuracy: 96\n",
      "Loss: 0.1105051189661026, accuracy: 96\n",
      "Loss: 0.12324003875255585, accuracy: 96\n",
      "Loss: 0.11057820171117783, accuracy: 96\n",
      "Loss: 0.10865706950426102, accuracy: 97\n",
      "Loss: 0.10824849456548691, accuracy: 96\n",
      "Loss: 0.11275696009397507, accuracy: 96\n",
      "Loss: 0.10923698544502258, accuracy: 96\n",
      "Loss: 0.11321546882390976, accuracy: 96\n",
      "Loss: 0.10587245225906372, accuracy: 96\n",
      "Loss: 0.11644107848405838, accuracy: 96\n",
      "Loss: 0.10572720319032669, accuracy: 96\n",
      "Loss: 0.10592672973871231, accuracy: 96\n",
      "Loss: 0.10624512284994125, accuracy: 96\n",
      "Loss: 0.12218598276376724, accuracy: 96\n",
      "Loss: 0.1034180149435997, accuracy: 96\n",
      "Loss: 0.114189513027668, accuracy: 96\n",
      "Loss: 0.10513007640838623, accuracy: 96\n",
      "Loss: 0.11694205552339554, accuracy: 96\n",
      "Loss: 0.10253959894180298, accuracy: 96\n",
      "Loss: 0.10903823375701904, accuracy: 96\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch, optimizer, model, train_loader, volatile=False, phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,optimizer, model, test_loader, volatile=False, phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa4d4396940>"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X10VPW97/H3FwLGIAgE0oo8hN4eFINGTECqhWotvZbbqrWHYmur0KrLVq3c3uqi7enR04e7tNqzevt0bmml1kqrXNFaXZaqpz70rFrkodgi4COxRJFJwrMiJOR7/9gzYQgzyTzsvWdCPq+1ZmVmz96//c1m+OQ3v9m/PebuiIhI/zGg1AWIiEi8FPwiIv2Mgl9EpJ9R8IuI9DMKfhGRfkbBLyLSzyj4RUT6GQW/iEg/o+AXEelnKkpdQCajRo3y2traUpchItJnrFmzptXdR+eyblkGf21tLatXry51GSIifYaZvZbruhrqERHpZxT8IiL9jIJfRKSfKcsxfhGJX3t7O83NzbzzzjulLkV6UFlZydixYxk0aFDBbSj4RQSA5uZmhg4dSm1tLWZW6nIkA3enra2N5uZmJk6cWHA7OQ31mNn1ZrbezJ43s4XJZfea2brkrcnM1mXZ9nwze8HMXjazRQVXKiKReuedd6iurlbolzEzo7q6uuh3Zb32+M1sCnAlMB04AKwws4fdfV7aOt8DdmXYdiDwY2A20AysMrPfufuGoqoWkUgo9MtfGP9GufT4JwMr3f1td+8AngIuTivCgE8Cv8mw7XTgZXd/1d0PAPcAFxZdtUhf9cc/wsaNpa5C+rlcgn89MNPMqs2sCpgDjEt7fiawzd1fyrDticCWtMfNyWVHMLOrzGy1ma1uaWnJrXqRvmb+fPjmN0tdRVnauXMnP/nJTwrads6cOezcuTPkio5evQa/u28EbgUeBVYA64CDaat8isy9/by4+2J3b3T3xtGjc5p1LNK3uMObb8K2baWuJBRLl0JtLQwYEPxcurS49noK/o6Ojh63feSRRxg+fHhxBUTA3ens7Cx1GUfI6cNdd7/D3RvcfRawA3gRwMwqCIZ97s2y6esc/u5gbHKZSP+zaxe0t0MiUepKirZ0KVx1Fbz2WvD37LXXgsfFhP+iRYt45ZVXOP3007nhhht48sknmTlzJhdccAGnnHIKABdddBENDQ3U1dWxePHirm1ra2tpbW2lqamJyZMnc+WVV1JXV8eHP/xh9u3bd8S+HnroIc4880ymTp3Khz70IbYl/xjv3buXBQsWcOqpp3LaaaexfPlyAFasWMEZZ5xBfX095513HgA333wzt99+e1ebU6ZMoampiaamJk466SQuu+wypkyZwpYtW/jCF75AY2MjdXV13HTTTV3brFq1irPOOov6+nqmT5/Onj17mDVrFuvWHTpX5v3vfz/PPfdc4Qc2E3fv9QbUJH+OBzYBw5OPzwee6mG7CuBVYCIwGHgOqOttfw0NDS5y1HnhBXdwHz261JVktGHDhpzXnTAh+FW63yZMKHz/mzdv9rq6uq7HTzzxhFdVVfmrr77ataytrc3d3d9++22vq6vz1tbWZD0TvKWlxTdv3uwDBw70v/71r+7uPnfuXP/Vr351xL62b9/unZ2d7u7+s5/9zL/85S+7u/uNN97o119//WHrJRIJHzt2bFcdqRpuuukmv+2227rWraur882bN/vmzZvdzPyZZ545ou6Ojg7/wAc+4M8995zv37/fJ06c6M8++6y7u+/atcvb29v9zjvv7KrhhRde8Ex5mOnfCljtOeS5u+d8Hv9yM6sG2oFr3D01mHYJ3YZ5zGwM8HN3n+PuHWZ2LfAHYCCwxN2fL+xPlEgfl+rpt7bCwYMwcGBp6ynCP/6R3/JCTZ8+/bDz1X/wgx/wwAMPALBlyxZeeuklqqurD9tm4sSJnH766QA0NDTQ1NR0RLvNzc3MmzePrVu3cuDAga59PP7449xzzz1d640YMYKHHnqIWbNmda0zcuTIXuueMGECM2bM6Hq8bNkyFi9eTEdHB1u3bmXDhg2YGSeccALTpk0DYNiwYQDMnTuXb33rW9x2220sWbKE+fPn97q/fOUU/O4+M8vy+RmWvUHwAXDq8SPAIwXWJ3L0SAW/O7S1QU1NaespwvjxwfBOpuVhGjJkSNf9J598kscff5xnnnmGqqoqzjnnnIznsx9zzDFd9wcOHJhxqOe6667jy1/+MhdccAFPPvkkN998c961VVRUHDZ+n15Let2bN2/m9ttvZ9WqVYwYMYL58+f3eB5+VVUVs2fP5sEHH2TZsmWsWbMm79p6o2v1iMQlfWy/j4/zf+c7UFV1+LKqqmB5oYYOHcqePXuyPr9r1y5GjBhBVVUVmzZt4i9/+UvB+9q1axcnnhicYPjLX/6ya/ns2bP58Y9/3PV4x44dzJgxg6effprNmzcDsH37diD4XGHt2rUArF27tuv57nbv3s2QIUM4/vjj2bZtG7///e8BOOmkk9i6dSurVq0CYM+ePV0fYl9xxRV86UtfYtq0aYwYMaLg3zMbBb9IXI6i4L/0Uli8GCZMALPg5+LFwfJCVVdXc/bZZzNlyhRuuOGGI54///zz6ejoYPLkySxatOiwoZR83XzzzcydO5eGhgZGjRrVtfxf/uVf2LFjB1OmTKG+vp4nnniC0aNHs3jxYi6++GLq6+uZNy+Yu/qJT3yC7du3U1dXx49+9CMmTZqUcV/19fVMnTqVk08+mU9/+tOcffbZAAwePJh7772X6667jvr6embPnt31TqChoYFhw4axYMGCgn/HnljwmUB5aWxsdH0Rixx1rr0WUr3J3/wGLrmktPV0s3HjRiZPnlzqMgR44403OOecc9i0aRMDBhzZP8/0b2Vma9y9MZf21eMXiUsiAakPBvt4j1+ic9ddd3HmmWfyne98J2Poh0HBLxKXRAJOPjmY8aTglywuu+wytmzZwty5cyPbh4JfJC4tLXDCCTBqVHBfpEQU/CJxSSSCUzhratTjl5JS8IvEoaPj0Ln7Cn4pMQW/SBza2oKJWwp+KQMKfpE4pIJewR+q4447rtQl9EkKfpE4dA/+3btBX2re5/V2uehypeAXiUP34Aed2dPNokWLDrtcQuqyx3v37uW8887jjDPO4NRTT+XBBx/sta1sl2/OdHnlbJdiTn83cd9993VdLG3+/PlcffXVnHnmmdx44408++yzvO9972Pq1KmcddZZvPDCCwAcPHiQr3zlK0yZMoXTTjuNH/7wh/zxj3/koosu6mr3scce4+Mf/3jhB61AuV6dU0SKkSn4EwkYNy77NqW0cCGkXRM+FKefDt//ftan582bx8KFC7nmmmuA4IqWf/jDH6isrOSBBx5g2LBhtLa2MmPGDC644IIev3t2yZIljBw5kn379jFt2jQ+8YlP0NnZyZVXXsnTTz/NxIkTu665861vfYvjjz+ev//970BwfZ7eNDc38+c//5mBAweye/du/vSnP1FRUcHjjz/O1772NZYvX87ixYtpampi3bp1VFRUsH37dkaMGMEXv/hFWlpaGD16NL/4xS/43Oc+l89RDIWCXyQOiURwGebhwyH1DXMa5z/M1KlTSSQSvPHGG7S0tDBixAjGjRtHe3s7X/va13j66acZMGAAr7/+Otu2bePd73531rYyXb65paUl4+WVM12KuTdz585lYPKy2rt27eLyyy/npZdewsxob2/vavfqq6+moqLisP199rOf5e6772bBggU888wz3HXXXfkeqqIp+EXikEgEgT9gQN8Y6umhZx6luXPnct999/Hmm292XQxt6dKltLS0sGbNGgYNGkRtbW2PlzXO9fLNvUl/R9F9+/TLLn/jG9/g3HPP5YEHHqCpqYlzzjmnx3YXLFjAxz72MSorK5k7d27XH4Y4aYxfJA4tLYcCP32oRw4zb9487rnnHu67776uSxbs2rWLmpoaBg0axBNPPMFrmb4IIE22yzdnu7xypksxA7zrXe9i48aNdHZ2dr17yLa/1CWe77zzzq7ls2fP5qc//WnXB8Cp/Y0ZM4YxY8bw7W9/O7Krb/ZGwS8Sh9SsXYChQ+GYYxT8GdTV1bFnzx5OPPFETjjhBAAuvfRSVq9ezamnnspdd93FySef3GMb2S7fnO3yypkuxQxwyy238NGPfpSzzjqrq5ZMbrzxRr761a8yderUw87yueKKKxg/fjynnXYa9fX1/PrXv+567tJLL2XcuHEluxqqLsssEof3vhfOPPPQt5GPHw8f/CCk9RBLTZdljs+1117L1KlT+fznP1/Q9sVelllj/CJxSO/xgyZx9WMNDQ0MGTKE733veyWrQcEvErV9+2DPHgW/AETyHbr50hi/SNRSZ+/0geAvx6FfOVwY/0YKfpGopU/eSkkFfxkFbWVlJW1tbQr/MubutLW1UVlZWVQ7GuoRiVoq+FMTtyAI/v37gyGgYcNKU1c3Y8eOpbm5mZZynl8gVFZWMnbs2KLaUPCLRC1Tjz999m6ZBP+gQYO6ZrXK0U1DPSJRyzbUA+U9e1eOWgp+kai1tMCxx0LaNH/N3pVSUvCLRC11Dn/61SQV/FJCCn6RqHWfvAW6QqeUlIJfJGqZgr+yMvhQV8EvJaDgF4lapuCHsp3EJUe/nILfzK43s/Vm9ryZLUxbfp2ZbUou/26Wbf9n8vn1ZvYbMytu5oFIX+Ku4Jey0+t5/GY2BbgSmA4cAFaY2cPAOOBCoN7d95vZEa9sMzsR+BJwirvvM7NlwCXAneH9CiJlbPduOHAge/C//HL8NUm/l0uPfzKw0t3fdvcO4CngYuALwC3uvh/A3bN1XSqAY82sAqgC3ii+bJE+ItOs3RT1+KVEcgn+9cBMM6s2sypgDkFvf1Jy+Uoze8rMpnXf0N1fB24H/gFsBXa5+6PhlS9S5jJN3kqpqYHWVujsjLcm6fd6DX533wjcCjwKrADWAQcJevIjgRnADcAy6/a192Y2gmA4aCIwBhhiZp/JtB8zu8rMVpvZal0rRI4aPQX/6NFB6Ce/kk8kLjl9uOvud7h7g7vPAnYALwLNwP0eeBboBEZ12/RDwGZ3b3H3duB+4Kws+1js7o3u3jg609tikb4o0yWZUzSJS0ok17N6apI/xxOM7/8a+C1wbnL5JGAw0Npt038AM8ysKvlu4DxgYzili/QBvY3xp68jEpNcr8653MyqgXbgGnffaWZLgCVmtp7gbJ/L3d3NbAzwc3ef4+4rzew+YC3QAfwVWBzB7yFSnhIJGD4cBg8+8jkFv5RITsHv7jMzLDsAHDFe7+5vEHwAnHp8E3BTETWK9F3ZzuEHBb+UjGbuikSpp+Cvrg4u3Kbgl5gp+EWi1FPwDxwIo0Yp+CV2Cn6RKPUU/KBJXFISCn6RqBw8GEzQ6un0ZAW/lICCXyQqbW3BRdp66/FrwqLETMEvEpWeZu2mjB6tHr/ETsEvEpWeZu2m1NTAzp3BFTxFYqLgF4lKLj3+1HMa7pEYKfhFopJP8Gu4R2Kk4BeJSiIBAwbAyJHZ11HwSwko+EWikkgEH94O6OG/mYJfSkDBLxKV3iZvgYJfSkLBLxKVVI+/J8OGBVfuVPBLjBT8IlHJpcdvpklcEjsFv0hUcgl+0GUbJHYKfpEo7N8Pu3fnFvyavSsxU/CLRCGXWbsp6vFLzBT8IlHIZfJWSir43aOtSSRJwS8ShXyDf98+eOutaGsSSVLwi0Qh3+BP30YkYgp+kSgo+KWMKfhFopBIQGUlHHdc7+sq+CVmCn6RKKRm7Zr1vq6CX2Km4BeJQq6Tt+DQZR00e1diouAXiUI+wX/ssTB0qHr8EhsFv0gU8gl+0OxdiZWCXyRs7sGwTT7Br9m7EiMFv0jY9u6Fd95R8EvZUvCLhC2fc/hTFPwSIwW/SNgKDf6WFujsjKYmkTQKfpGwFRr8Bw/Cjh3R1CSSJqfgN7PrzWy9mT1vZgvTll9nZpuSy7+bZdvhZnZfcr2NZva+sIoXKUup4O/taxfTaRKXxKiitxXMbApwJTAdOACsMLOHgXHAhUC9u+83s2zdm/8DrHD3fzazwUBVOKWLlKligr+lBSZPDr8mkTS9Bj8wGVjp7m8DmNlTwMVAI3CLu+8HcPcjuipmdjwwC5ifXOcAwR8PkaNXIhF8iXplZe7bqMcvMcplqGc9MNPMqs2sCphD0NuflFy+0syeMrNpGbadCLQAvzCzv5rZz81sSKadmNlVZrbazFa3aOq69GX5Tt4CBb/Eqtfgd/eNwK3Ao8AKYB1wkODdwkhgBnADsMzsiCtSVQBnAP/h7lOBt4BFWfaz2N0b3b1xdD5vkUXKTSHBX119aFuRiOX04a673+HuDe4+C9gBvAg0A/d74FmgExjVbdNmoNndVyYf30fwh0Dk6JXvrF2Aioog/BX8EoNcz+qpSf4cTzC+/2vgt8C5yeWTgMFAa/p27v4msMXMTkouOg/YEErlIuWqkB4/aBKXxCaXD3cBlptZNdAOXOPuO81sCbDEzNYTfGB7ubu7mY0Bfu7uc5LbXgcsTZ7R8yqwIOTfQaR8dHYW1uMHBb/EJqfgd/eZGZYdAD6TYfkbBB8Apx6vIzgDSOTot317EP6FBv/f/hZ+TSLdaOauSJgKmbWboh6/xETBLxKmQiZvpdTUBJdsaG8PtyaRbhT8ImEqtscP0Nra83oiRVLwi4QpjODXcI9ETMEvEqZEAswOTcjKh4JfYqLgFwlTIgGjRsHAgflvm/pcQMEvEVPwi4Sp0HP4QT1+iY2CXyRMhc7aBRg+PLh0g4JfIqbgFwlTMcFvpnP5JRYKfpEwFRP8oOCXWCj4RcJy4ADs3Kngl7Kn4BcJS+oLhIr5PomamkPtiEREwS8SlmImb6Woxy8xUPCLhCWs4H/rreAmEhEFv0hYwgp+0HCPRErBLxKWMIJfs3clBgp+kbC0tMDgwTBsWOFtaPauxEDBLxKW1Dn8ZoW3oeCXGCj4RcJS7OQt0FCPxELBLxKWMIJ/yJDgpuCXCCn4RcKSSBQ3eStF5/JLxBT8ImFwD6fHD5q9K5FT8IuE4a23YN++8IJfPX6JkIJfJAxhnMOfouCXiCn4RcIQRfC7F9+WSAYKfpEwhB38HR3BJZ5FIqDgFwlD6sPYMIJf5/JLxBT8ImFIhXRYp3OmtykSMgW/SBgSCRg6FI49tvi2FPwSMQW/SBjCOocfFPwSuZyC38yuN7P1Zva8mS1MW36dmW1KLv9uD9sPNLO/mtnDYRQtUnbCmrULMGpU8FOTuCQiFb2tYGZTgCuB6cABYEUywMcBFwL17r7fzHrq7lwPbASKuF6tSBlLJKC2Npy2Bg2CkSPV45fI5NLjnwysdPe33b0DeAq4GPgCcIu77wdw94yvUjMbC/wP4OfhlCxShsIc6gFN4pJI5RL864GZZlZtZlXAHILe/qTk8pVm9pSZTcuy/feBG4HOUCoWKTedncGwjIJf+oheg9/dNwK3Ao8CK4B1wEGCYaKRwAzgBmCZ2eHfQGFmHwUS7r6mt/2Y2VVmttrMVrdobFP6kh074OBBBb/0GTl9uOvud7h7g7vPAnYALwLNwP0eeJagRz+q26ZnAxeYWRNwD/BBM7s7yz4Wu3ujuzeODutDMpE4hDlrN0XBLxHK9ayemuTP8QTj+78Gfgucm1w+CRgMtKZv5+5fdfex7l4LXAL80d0/E1r1IuUgzFm7KaNHQ1tbcOkGkZDleh7/cjPbADwEXOPuO4ElwHvMbD1Bb/5yd3czG2Nmj0RUr0j5iarHD9Da2vN6IgXo9XROAHefmWHZAeCI3ru7v0HwAXD35U8CT+ZdoUi5izL4Ewl497vDa1cEzdwVKV4iAWZQXR1em5q9KxFS8IsUK5EIJlxV5PQGOjep4NcZbhIBBb9IscKevAXq8UukFPwixYoi+IcPD95BKPglAgp+kWJFEfwDBgSndCr4JQIKfpFiRRH8oElcEhkFv0gx2tuDSzYo+KUPUfCLFCM1wSqK4NdQj0REwS9SjCgmb6Woxy8RUfCLFCPq4N+7F/btC79t6dcU/CLFSAV/FFeU1SQuiYiCX6QYUff40/chEhIFv0gxEolgotXw4eG3reCXiCj4RYqROof/8C+fC4eCXyKi4BcpRlSTt0DBL5FR8IsUI8rgHzIEqqoU/BI6Bb9IMaIMftC5/BIJBb9IMVpaog1+zd6VCCj4RQr11lvBTT1+6WMU/CKFSk2sUvBLH6PgFylUlLN2U2pqgj8w7tHtQ/odBb9IoaKctZtSUwMHDsDu3dHtQ/odBb9IoeIK/vR9iYRAwS9SqLiGetL3JRICBb9IoRKJYJLVkCHR7UPBLxFQ8IsUKurJW6Dgl0go+EUKFUfwjxp1aF8iIVHwixQq6lm7AIMHB5d8VvBLiBT8IoWKo8cPmsQloVPwixTCPQjjKM/oSUlN4hIJiYJfpBA7d0JHh3r80iflFPxmdr2ZrTez581sYdry68xsU3L5dzNsN87MnjCzDcl1rg+zeJGSiWPyVoqCX0JW0dsKZjYFuBKYDhwAVpjZw8A44EKg3t33m1mm/wEdwP9y97VmNhRYY2aPufuG8H4FkRKIO/hbW+HgQRg4MPr9yVEvlx7/ZGClu7/t7h3AU8DFwBeAW9x9P4C7H9Elcfet7r42eX8PsBE4MaziRUom7uB3h7a26Pcl/UIuwb8emGlm1WZWBcwh6O1PSi5faWZPmdm0nhoxs1pgKrAyy/NXmdlqM1vdog+ypNzFHfzp+xQpUq/B7+4bgVuBR4EVwDrgIMEw0UhgBnADsMzMLFMbZnYcsBxY6O4ZLzPo7ovdvdHdG0fHcaaESDFSIZyaYBUlBb+ELKcPd939DndvcPdZwA7gRaAZuN8DzwKdwBH/C8xsEEHoL3X3+8MrXaSEEgkYORIGDYp+Xwp+CVmvH+4CmFmNuyfMbDzB+P4MgqA/F3jCzCYBg4HWbtsZcAew0d3/PdTKRUopjlm7Kal3wAp+CUmu5/EvN7MNwEPANe6+E1gCvMfM1gP3AJe7u5vZGDN7JLnd2cBngQ+a2brkbU7Yv4RI7OKatQvBO4sBAxT8EpqcevzuPjPDsgPAZzIsf4PgA2Dc/b+AjOP+In1aIgGnnBLPvgYMCHr9OulBQqKZuyKFiLPHD5rEJaFS8Ivkq6MjOKdewS99lIJfJF+tyXMYFPzSRyn4RfIV5+StFAW/hEjBL5KvUgX/7t3wzjvx7VOOWgp+kXyVKvhBZ/ZIKBT8IvkqZfBruEdCoOAXyVdLC1RUBN+FGxfN3pUQKfhF8pX6ysUBMf730VCPhEjBL5KvuL5rN52GeiRECn6RfMU9axfguOOgslLBL6FQ8IvkqxTBb6Zz+SU0Cn6RfJUi+EHBL6FR8Ivk4+23Ye9eBb/0aQp+kXykzqpR8EsfpuAXyUcpJm+lpILfPf59y1FFwS+Sj1IH//79sGdP/PuWo4qCXyQfpR7qAQ33SNEU/CL5SIVu3BO40vep2btSJAW/SD4SCTj2WBgyJP59q8cvIVHwi+QjdQ6/Wfz7VvBLSBT8Ivko1eQt0BU6JTQKfpF8lDL4jzkGjj9ewS9FU/CL5KOUwQ+axCWhUPCL5MpdwS9HBQW/SK527YL2dgW/9HkKfpFclXLWboqCX0Kg4BfJVSln7abU1EBrK3R2lq4G6fMU/CK5KuWs3ZTRo4PQ3769dDVIn6fgF8lVuQz1pNciUoCcgt/Mrjez9Wb2vJktTFt+nZltSi7/bpZtzzezF8zsZTNbFFbhIrErhx6/gl9C0Gvwm9kU4EpgOlAPfNTM3mtm5wIXAvXuXgfcnmHbgcCPgY8ApwCfMrNTQqy/y9KlUFsLAwYEP5cu7b9tlEMNR2Mbd96WYAfDqZ00uGR1PPxsEPzzzk2U/HiUuo1yqKGc2siLu/d4A+YCd6Q9/gZwI7AM+FAv274P+EPa468CX+1tnw0NDZ6Pu+92r6pyD060Dm5VVcHy/tZGOdRwtLZxD5/0TUwqaR3jj024g1/DD0t+PPQ6L5823N2B1d5LtqZu5r18m4+ZTQYeTIb4PuA/gdXAzOTy84F3gK+4+6pu2/4zcL67X5F8/FngTHe/tqd9NjY2+urVq3P7y0XwF/K112AVjRzLvq7lgwbBpH/KrY0XXwpO0e6ur7VRDjUcrW1M4DXWcToz+S8AJkyApqbc2ki9RrvLt40trx3kAINpo5oWgiGnvnxMC22jHGqIoo02qvkATwP5vTYAzGyNuzfmsm5Fbyu4+0YzuxV4FHgLWAccTG47EpgBTAOWmdl7vLe/JNmLvgq4CmD8+PF5bfuPfwQ/N3Eyx7D/0BPtMCnHgaXnNmR5oo+1UQ41HK1tbOAU7mVe1/LU6y4X2dbNtw1nIDfxb9Tz3KEn+vAxLbSNcqghijZ2Mrzrfj6vjbzl+tYgdQP+N/BFYAVwbtryV4DR3daNZahnwoTD3yalbhMm9L82yqEGtaE2om6jHGoopzbcPa+hnlzDvib5czywCRgOXA18M7l8ErAFgqGjtO0qgFeBicBg4Dmgrrf9aYy/8DbKoQa1oTaibqMcaiinNtyjCf4/ARuSwX1ectlg4G5gPbAW+GBy+RjgkbRt5wAvJt8RfD2X/eUb/KmDN2GCu1nwM9+DdjS1UQ41qA21EXUb5VBDObWRT/D3+uFuKeT74a6ISH+Xz4e7mrkrItLPKPhFRPoZBb+ISD+j4BcR6WcU/CIi/UxZntVjZi1AhgnuZWMU0FrqInLQV+qEvlOr6gxfX6m13Ouc4O45XTq2LIO/3JnZ6lxPmyqlvlIn9J1aVWf4+kqtfaXOXGioR0Skn1Hwi4j0Mwr+wiwudQE56it1Qt+pVXWGr6/U2lfq7JXG+EVE+hn1+EVE+hkFfxZmNs7MnjCzDckvk78+wzrnmNkuM1uXvP1riWptMrO/J2s44up2FvhB8gvv/2ZmZ5SozpPSjtU6M9ttZgu7rVOSY2pmS8wsYWbr05aNNLPHzOyl5M8RWba9PLnOS2Z2eQnqvM3MNiX/bR8ws+FZtu3xdRJTrTeb2etp/75zsmx7vpm9kHzNLipBnfem1dhkZuuybBvrMQ1Nrpfx7G834ATgjOT9oQSXlj6l2zoBkLf5AAADkUlEQVTnAA+XQa1NwKgenp8D/B4wgm9MW1kGNQ8E3iQ497jkxxSYBZwBrE9b9l1gUfL+IuDWDNuNJPjOiZHAiOT9ETHX+WGgInn/1kx15vI6ianWmwm+prW318YrwHs49D0ep8RZZ7fnvwf8azkc07Bu6vFn4e5b3X1t8v4eYCNwYmmrKtiFwF0e+Asw3MxOKHFN5wGvuHtZTNRz96eB7d0WXwj8Mnn/l8BFGTb978Bj7r7d3XcAjxF8D3Vsdbr7o+7ekXz4F2BsVPvPR5ZjmovpwMvu/qq7HwDuIfi3iERPdZqZAZ8EfhPV/ktBwZ8DM6sFpgIrMzz9PjN7zsx+b2Z1sRZ2iAOPmtma5HcXd3ciwTekpTRT+j9il5D9P1M5HFOAd7n71uT9N4F3ZVin3I7t5wje3WXS2+skLtcmh6WWZBk+K6djOhPY5u4vZXm+XI5pXhT8vTCz44DlwEJ3393t6bUEQxX1wA+B38ZdX9L73f0M4CPANWY2q0R15MTMBgMXAP8vw9PlckwP48H7+rI+Bc7Mvg50AEuzrFIOr5P/AP4bcDqwlWAYpZx9ip57++VwTPOm4O+BmQ0iCP2l7n5/9+fdfbe7703efwQYZGajYi4Td389+TMBPEDwVjnd68C4tMdjk8tK5SPAWnff1v2JcjmmSdtSQ2LJn4kM65TFsTWz+cBHgUuTf6SOkMPrJHLuvs3dD7p7J/CzLDWUyzGtAC4G7s22Tjkc00Io+LNIju3dAWx093/Pss67k+thZtMJjmdbfFWCmQ0xs6Gp+wQf9K3vttrvgMuSZ/fMAHalDWGUQtZeVDkc0zS/A1Jn6VwOPJhhnT8AHzazEclhiw8nl8XGzM4HbgQucPe3s6yTy+skct0+W/p4lhpWAf9kZhOT7w4vIfi3iNuHgE3u3pzpyXI5pgUp9afL5XoD3k/w1v5vwLrkbQ5wNXB1cp1rgecJzjr4C3BWCep8T3L/zyVr+XpyeXqdBvyY4EyJvwONJTyuQwiC/Pi0ZSU/pgR/iLYC7QRjyp8HqoH/BF4CHgdGJtdtBH6etu3ngJeTtwUlqPNlgjHx1Ov0/ybXHQM80tPrpAS1/ir5GvwbQZif0L3W5OM5BGfSvRJ1rZnqTC6/M/W6TFu3pMc0rJtm7oqI9DMa6hER6WcU/CIi/YyCX0Skn1Hwi4j0Mwp+EZF+RsEvItLPKPhFRPoZBb+ISD/z/wF1wSM9k/74mAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,len(train_accuracy)+1),train_accuracy,'bo',label = 'train accuracy')\n",
    "plt.plot(range(1,len(val_accuracy)+1),val_accuracy,'r',label = 'val accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "* Reusing pre-trained algorithm on a dataset similar to what we are using. Training from scratch not required. \n",
    "* Freeze most of the layers and fine tune params for some of the layers only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/kushashwa/.torch/models/vgg16-397923af.pth\n",
      "100%|██████████| 553433881/553433881 [00:18<00:00, 30166213.80it/s]\n"
     ]
    }
   ],
   "source": [
    "vgg = models.vgg16(pretrained=True) # load the pretrained model VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace)\n",
      "    (5): Dropout(p=0.5)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# let's look at the VGG network\n",
    "print(vgg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above summary of VGG16 Model, there are 2 Sequential models:\n",
    "\n",
    "**1. Features:**\n",
    "    - Has the layers that we are going to freeze\n",
    "**2. Classifier:**\n",
    "    - Has the layers that we are going to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze the weights for features layers \n",
    "for param in vgg.features.parameters(): \n",
    "    param.requites_grad = False\n",
    "    # prevent optimizer from updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fine tune the classifier layers\n",
    "# for dogs and cats?\n",
    "vgg.classifier[6].out_features = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(vgg.classifier.parameters(), lr=0.001, momentum=0.5) \n",
    "# only pass classifier parameters for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick Tips to improve accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing dropout from 0.2 to 0.5\n",
    "for layer in vgg.classifier.children():\n",
    "    if(type(layer) == nn.modules.dropout.Dropout):\n",
    "        layer.p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing data augmentation to add more data: flipping, mirroring, rotating with small angles\n",
    "train_transform = transforms.Compose([transforms.Resize((224, 224)), \n",
    "                                     transforms.RandomHorizontalFlip(), \n",
    "                                     transforms.RandomRotation(0.2),\n",
    "                                     transforms.ToTensor(), \n",
    "                                     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Outputs from Intermediate Layers\n",
    "\n",
    "We can visualize outputs from Intermediate Layers, using PyTorch utility function: `register_forward_hook`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerActivations():\n",
    "    features=None\n",
    "    \n",
    "    def __init__(self,model,layer_num):\n",
    "        self.hook = model[layer_num].register_forward_hook(self.hook_fn)\n",
    "    \n",
    "    def hook_fn(self,module,input,output):\n",
    "        self.features = output.cpu().data.numpy()\n",
    "    \n",
    "    def remove(self):\n",
    "        self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_out = LayerActivations(vgg.features, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = []\n",
    "for i in range(64):\n",
    "    img.append(np.random.randn(16, 16, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 16, 16, 3)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.array(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is the image created.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape((64, 16, 16, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5cf7f02ba8>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(img[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = img.reshape((64, 3, 16, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.from_numpy(img).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This wasn't working on my PC, lack of free RAM you know? ;) \n",
    "\n",
    "Thanks to Google Collabs :D Implemented it here: (on a random input image)\n",
    "\n",
    "https://colab.research.google.com/drive/12UH6rwswmxIuvIvvHT-wjB0LNVj-Txw9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing weights of Intermediate Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_weights = vgg.state_dict()['features.0.weight'].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x2160 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# directly taken from the book\n",
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(left=0,right=1,bottom=0,top=0.8,hspace=0,wspace=0.2)\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(12,6,i+1,xticks=[],yticks=[])\n",
    "    plt.imshow(cnn_weights[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "latex_metadata": {
   "author": "Kushashwa Ravi Shrimali",
   "title": "Deep Learning in Computer Vision using PyTorch, Chapter 5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
